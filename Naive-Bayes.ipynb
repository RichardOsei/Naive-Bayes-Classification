{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Naive Bayes Classifier</h1>\n",
    "\n",
    "Naive Bayes is a simple yet powerful probabilistic classifier based on Bayes' theorem with an assumption of independence between features.Despite its simplicity, Naive Bayes often performs surprisingly well in practice, particularly in text classification and other domains where the assumption of independence holds reasonably well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data source: Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/illinois-cse/data-fa14/gh-pages/data/iris.csv')\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]]\n",
    "y = df['species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "        self.classes = None\n",
    "        self.class_prior_probs = {}\n",
    "        self.feature_probs = {}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        for c in self.classes:\n",
    "            X_c = X[y == c]\n",
    "            self.class_prior_probs[c] = (len(X_c) + self.alpha) / (len(X) + self.alpha * len(self.classes))\n",
    "            self.feature_probs[c] = (np.sum(X_c, axis=0) + self.alpha) / (np.sum(X_c) + self.alpha * X.shape[1])\n",
    "\n",
    "    def _predict_single(self, x):\n",
    "        posteriors = {c: np.log(self.class_prior_probs[c]) +\n",
    "                          np.sum(np.log(self.feature_probs[c]) * x) for c in self.classes}\n",
    "        return max(posteriors, key=posteriors.get)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = [self._predict_single(x) for x in X]\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 1, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 1, 0, 2, 1, 2, 2, 2, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Train Naive Bayes classifier\n",
    "nb_classifier = NaiveBayes()\n",
    "nb_classifier.fit(X_train.to_numpy(), y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = nb_classifier.predict(X_test.to_numpy())\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9\n",
      "Precision: 0.925\n",
      "Recall: 0.9\n",
      "F1-score: 0.899248120300752\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test, predictions, average='weighted')\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_test, predictions, average='weighted')\n",
    "\n",
    "# Calculate F1-score\n",
    "f1 = f1_score(y_test, predictions, average='weighted')\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the evaluation metrics:\n",
    "\n",
    "* Accuracy: The classifier achieves an accuracy of 90%, indicating that 90% of the samples in the test set are correctly classified by the model.\n",
    "\n",
    "* Precision: The precision score of 92.5% suggests that when the classifier predicts a class label, it is correct approximately 92.5% of the time on average.\n",
    "\n",
    "* Recall: The recall score of 90% indicates that the classifier correctly identifies 90% of the samples belonging to each class.\n",
    "\n",
    "* F1-score: The F1-score of 89.9% is the weighted average of precision and recall, providing a balanced measure of the classifier's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Conclusion: Overall, the Naive Bayes classifier demonstrates a good performance on the test data, with high accuracy, precision, recall, and F1-score. However, further analysis may be required to understand specific areas of improvement or potential issues, such as class imbalances or misclassifications in certain classes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
